{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12cc479-c778-4c7e-9baa-67d92f4ec978",
   "metadata": {},
   "source": [
    "Assignment-11: Write a Python script that:\n",
    "1. Tokenizes a sample paragraph into words and sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b187d491-2b49-4b9b-8e18-1699d08506b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\anaconda4\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\anaconda4\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\anaconda4\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\anaconda4\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\anaconda4\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\anaconda4\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e3e17cf-125d-4e2a-b00f-a00598963e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91fdbb45-be8a-4061-bc13-6cbb8e8da9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      "['\\n    Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful.']\n",
      "\n",
      "Tokenized Words in Sentence: \n",
      "    Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.']\n",
      "\n",
      "Tokenized Words in Sentence: The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful.\n",
      "['The', 'ultimate', 'goal', 'of', 'NLP', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'way', 'that', 'is', 'both', 'valuable', 'and', 'meaningful', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample paragraph\n",
    "sample_paragraph = \"\"\"\n",
    "    Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. \n",
    "    The ultimate goal of NLP is to enable computers to understand, interpret, and generate human language in a way that is both valuable and meaningful.\n",
    "    \"\"\"\n",
    "\n",
    "# Tokenize the paragraph into sentences\n",
    "sentences = sent_tokenize(sample_paragraph)\n",
    "print(\"Tokenized Sentences:\")\n",
    "print(sentences)\n",
    "print()\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    print(f\"Tokenized Words in Sentence: {sentence}\")\n",
    "    print(words)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f52160-f077-439e-b0b7-a46e001ac2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f2979-17bc-426a-904c-d80c031e4342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644bee34-1f43-4078-be8f-9b8e4b84ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
